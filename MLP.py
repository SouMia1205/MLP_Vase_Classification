# pylint: disable=all
import numpy as np  


class MLP:
    def __init__(self, n_entr√©es, couche_cach√©s,n_sorties):
        self.n_entr√©es = n_entr√©es  # nombre d'entr√©es (sans le biais)
        self.couche_cach√©s = couche_cach√©s  # Liste du nombre de neurones dans chaque couche cach√©e
        self.n_sorties = n_sorties  # nombre de neurones de sortie.
        
        # Initialisation des poids et biais
        self.poids = []  # Liste des matrices de poids
        self.biais = []  # Liste des vecteurs de biais
        
        # Couche d'entr√©e  √† premi√®re couche cach√©e
        self.poids.append(np.random.randn(couche_cach√©s[0], n_entr√©es))
        self.biais.append(np.random.randn(couche_cach√©s[0], 1))   # chaque neuron dans la premi√®re couche cach√©e a un biais associ√©.
        
        # (nombre de neurones dans la couche suivante) x (nombre de neurones dans la couche actuelle + 1) (le +1 est pour le biais).
        for i in range (1, len(couche_cach√©s)):
            self.poids.append(np.random.randn(couche_cach√©s[i], couche_cach√©s[i-1]))
            self.biais.append(np.random.randn(couche_cach√©s[i], 1))
        
        # Derni√®re couche cach√©e √† la couche de sortie
        self.poids.append(np.random.randn(n_sorties, couche_cach√©s[-1]))
        self.biais.append(np.random.randn(n_sorties, 1))
    
    
    # Fonction d'activation sigmoid
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    # D√©riv√©e de la sigmoid 
    def deriv√©_sigmoid(self, x):
        return x * (1 - x) 
    
    # Fonction de propagation vers l'avant (propager les entr√©es vers la sortie √† travers le r√©seau)
    # ajouter variable pour renvoie toutes les valeurs interm√©diaires pour retropropagation
    def forward(self, X):
        """
        Propagation vers l'avant √† travers le r√©seau.
        :X -- Donn√©es d'entr√©e (n_entr√©es, n_exemples)
        :return -- Sortie du r√©seau apr√©s activation.
        """
        A = X  # Initialisation de la sortie de la couche d'entr√©e
        activa = [X]  # Stocker les actiovations pour la retropropagation
        Valeurs_Z = [] # Stocker les z pour calculer les gradient 
        for poids, biais in zip(self.poids, self.biais):
            Z = np.dot(poids, A) + biais    # Calcul lin√©aire  Z = WX + b
            A = self.sigmoid(Z)   # Applique la fonction d'activation sigmoid
            Valeurs_Z.append(Z)
            activa.append(A)
        return A , activa, Valeurs_Z   # Sortie finale du r√©seau + les valeurs 
    
    # Fonction de retropropagation du gradient (MAJ les poids avec la descente de gradient)
    def backward(self, X , Valeurs_vrais):
        """
        Valeurs_vrais -- Valeurs vraies de sortie 
        X -- Donn√©es d'entr√©e 
        """
        # propagation avant pour obtenir les activations et les valeurs Z
        valeur_predite, activa, Valeurs_Z = self.forward(X)
        
        # Calcul de l'erreur de sortie (entre la sortie pr√©dite et la sortie vraie)
        erreur = Valeurs_vrais - valeur_predite 
        
        # Calcul du gradient de l'erreur pour la couche de sortie
        gradient_sortie = erreur * self.deriv√©_sigmoid(valeur_predite)  # dL/dA * dA/dZ
        
        # Mise √† jour les poids de la derni√®re couche vers la couche pr√©c√©dente
        for i in range(len(self.poids) -1, -1, -1):
            dP = np.dot(gradient_sortie, activa[i].T)  # dL/dA * dA/dZ * dZ/dW  gradient des poids
            self.poids[i] -= dP * 0.01  # Mise √† jour des poids
            dB = np.sum(gradient_sortie, axis=1, keepdims=True)  # dL/dA * dA/dZ * dZ/dB  gradient des biais
            self.biais[i] -= dB  * 0.01 # Mise √† jour des biais (0.01 est le taux d'apprentissage)
            # calculer le gradient pour la couche pr√©c√©dente 
            if i > 0:
                gradient_sortie = np.dot(self.poids[i].T, gradient_sortie) * self.deriv√©_sigmoid(activa[i])
                
    # Entra√Ænement du r√©seau
    def entrainement(self, X , Valeurs_vrais, n_iteration = 500):
        """_
        entrainer le modele MLP en utilisant la retropropagation
        X -- Donn√©es d'entr√©e
        Valeurs_vrais -- Valeurs vraies de sortie
        n_iteration -- Nombre d'it√©rations pour l'entra√Ænement
        """
        for j in range(n_iteration):
            self.backward(X, Valeurs_vrais)   # update les poids
            if j % 100 == 0:
                print(f"It√©ration {j} : Erreur = {np.mean((self.forward(X)[0] - Valeurs_vrais) ** 2)}")
                

# Example d'utilisation
mlp1 = MLP(n_entr√©es=2, couche_cach√©s=[3, 2], n_sorties=1)   # 2 entr√©es, 3 neurones dans la premi√®re couche cach√©e, 2 neurones dans la deuxi√®me couche cach√©e, et 1 neurone de sortie.
x_iputs = np.random.randn(2, 1)  # Une entr√©e avec 2 features
output = mlp1.forward(x_iputs)
    
print("Sortie du MLP est :",output)

mlp1.entrainement(x_iputs, np.array([[1]]), n_iteration=500)
# üîπ Test sur les donn√©es d'entra√Ænement
output = mlp1.forward(x_iputs)[0]
print("Sortie apr√®s entra√Ænement :", output)


# Donn√©es XOR
X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])
Y = np.array([[0, 1, 1, 0]])

# Initialisation du r√©seau
mlp_xor = MLP(n_entr√©es=2, couche_cach√©s=[2], n_sorties=1)

# Entra√Ænement
mlp_xor.entrainement(X, Y, n_iteration=10000, lr=0.1)

# Test du mod√®le
sortie = mlp_xor.forward(X)[0]
print("\nSortie apr√®s entra√Ænement :")
print(sortie)



data = np.loadtxt("data.txt") 